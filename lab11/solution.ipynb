{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModel, GPTNeoForCausalLM\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_code() -> str:\n",
    "    \"\"\"\n",
    "        Loads a Python written code from a given file.\n",
    "        \n",
    "        :return: A random code from the dataset.\n",
    "    \"\"\"\n",
    "    n = random.randint(1, 20)\n",
    "    code = ''\n",
    "    \n",
    "    with open(f\"data/code/{n}.txt\", 'r') as file:\n",
    "        code = file.read()\n",
    "        \n",
    "    return code\n",
    "    \n",
    "def load_request() -> str:\n",
    "    \"\"\"\n",
    "        Loads a code request from a given file.\n",
    "        \n",
    "        :rtype: str\n",
    "        :return: The code request from the file.\n",
    "    \"\"\"\n",
    "    n = random.randint(1, 11)\n",
    "    with open(f\"data/requests/{n}.txt\", 'r') as file:\n",
    "        return file.read()\n",
    "    \n",
    "def load_dataset(dataset_path: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "        Loads the dataset for the classification process.\n",
    "        \n",
    "        :param dataset_path: The path of the file where the dataset is saved.\n",
    "    \"\"\"\n",
    "    data = pd.read_csv(dataset_path, delimiter='\\\\')\n",
    "    \n",
    "    return data\n",
    "\n",
    "def split_data(data: pd.DataFrame, test_size: float = 0.2) -> tuple:\n",
    "    \"\"\"\n",
    "        Splits the data into training and testing sets.\n",
    "    \n",
    "        :param data: The dataset to be split\n",
    "        :param test_size: The proportion of the dataset to include in the test split\n",
    "        \n",
    "        :rtype: tuple\n",
    "        :return: A tuple containing the training and testing data and labels.\n",
    "    \"\"\"\n",
    "    \n",
    "    X = data['code']\n",
    "    y = data['label']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=42)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(\n",
    "    code: str, tokenizer: AutoTokenizer, model: AutoModel, device: torch.device\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates the embeddings for the code provided.\n",
    "\n",
    "    :param code: The code.\n",
    "    :param tokenizer: The tokenzizer.\n",
    "    :param model: The model.\n",
    "    :param device: The device that will generate the embeddings.\n",
    "    \"\"\"\n",
    "    inputs = tokenizer(code, return_tensors=\"pt\")\n",
    "    inputs = inputs.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "\n",
    "    embeddings = outputs.last_hidden_state\n",
    "\n",
    "    return embeddings.mean(dim=1)\n",
    "\n",
    "\n",
    "def get_embeddings(x_train, x_test, tokenizer, model, device):\n",
    "    x_train_embeddings = []\n",
    "    for code in x_train:\n",
    "        inputs = tokenizer(code, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "        x_train_embeddings.append(embeddings)\n",
    "\n",
    "    x_test_embeddings = []\n",
    "    for code in x_test:\n",
    "        inputs = tokenizer(code, return_tensors=\"pt\").to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1).squeeze().cpu().numpy()\n",
    "        x_test_embeddings.append(embeddings)\n",
    "\n",
    "    return np.array(x_train_embeddings), np.array(x_test_embeddings)\n",
    "\n",
    "\n",
    "def train_classifier(classifier: RandomForestClassifier, x_train, y_train):\n",
    "    classifier.fit(x_train, y_train)\n",
    "\n",
    "\n",
    "def classify_code(classifier: RandomForestClassifier, embeddings: list):\n",
    "    \"\"\"\n",
    "    Classifies the code provided.\n",
    "\n",
    "    :param classifier: The classifier used.\n",
    "    :param embeddings: The embeddings of the code.\n",
    "    \"\"\"\n",
    "    predicted_class_label = classifier.predict(embeddings)[0]\n",
    "\n",
    "    return predicted_class_label\n",
    "\n",
    "\n",
    "def generate_comments(\n",
    "    code: str,\n",
    "    tokenizer: AutoTokenizer,\n",
    "    model: AutoModel,\n",
    "    max_length: int = 500,\n",
    "):\n",
    "    \"\"\"\n",
    "    Generates comments for a given code snippet.\n",
    "\n",
    "    :param code: The code for which the comments will be generated.\n",
    "    :param tokenizer: Tokenizer for tokenizing the code provided.\n",
    "    :param model: The model that will generate comments for the code.\n",
    "    :return: The code with generated comments.\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    You are a software specification generator. Given the following code snippet, generate a detailed specification comment that includes:\n",
    "    1. Purpose of the code.\n",
    "    2. Short description of the logic of each function and method.\n",
    "    3. Description of the input parameters and the output.\n",
    "    4. Include complexity analysis if possible.\n",
    "    \n",
    "    Code Snippet:\n",
    "    {code}\n",
    "    \n",
    "    Specification:\n",
    "    \"\"\"\n",
    "\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors=\"pt\")\n",
    "    output = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,  # adjust for diversity\n",
    "        do_sample=True,\n",
    "        repetition_penalty=2.0,\n",
    "        num_return_sequences=1,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "\n",
    "    comments = tokenizer.decode(output[0], skip_special_tokens=True)\n",
    "    return comments\n",
    "\n",
    "def generate_from_request(request: str, tokenizer: AutoTokenizer, model: AutoModel, max_length=500):\n",
    "    prompt = f\"\"\"\n",
    "    You are a code generator. Given the following request, generate a code written in Python that will solve the request.\n",
    "    \n",
    "    Request:\n",
    "    {request}\n",
    "    \n",
    "    Code:\n",
    "    \"\"\"\n",
    "    \n",
    "    input_ids = tokenizer.encode(request, return_tensors='pt')\n",
    "    output_ids = model.generate(\n",
    "        input_ids,\n",
    "        max_length=max_length,\n",
    "        temperature=0.7,  # adjust for diversity\n",
    "        do_sample=True,\n",
    "        repetition_penalty=2.0,\n",
    "        num_return_sequences=1,\n",
    "        early_stopping=True,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "    )\n",
    "    code = tokenizer.decode(output_ids[0])\n",
    "    \n",
    "    return code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base code:\n",
      " def f(text: str):\n",
      "    words = text.split(\" \")\n",
      "\n",
      "    word = words[0]\n",
      "\n",
      "    for i in range(1, len(words)):\n",
      "        if words[i] > word:\n",
      "            word = words[i]\n",
      "\n",
      "    return word\n",
      "\n",
      "\n",
      "Embeddings: tensor([[-6.2189e-01, -1.8493e+00,  1.7276e+00,  9.5247e-01,  7.5159e-01,\n",
      "         -2.3681e-01, -1.7968e+00, -1.1511e-01,  1.4903e+00,  5.5977e-01,\n",
      "         -9.1783e-01, -5.5690e-01, -6.6448e-01,  4.1554e+00,  5.5137e-01,\n",
      "         -9.3710e-01,  3.3748e-02, -6.8097e-01, -2.0198e+00, -3.6352e-01,\n",
      "          1.1896e+00,  2.8250e+00,  5.6498e-01,  1.2844e+00, -1.1044e+00,\n",
      "          3.1458e-01,  4.7703e-01, -1.2139e+00,  6.9446e-01, -6.2384e-01,\n",
      "         -1.0959e+00, -1.1198e+00, -1.4293e+00, -1.3017e+00,  5.6370e-01,\n",
      "         -1.9252e+00,  8.8036e-02, -2.3315e+00,  1.1220e+00,  9.2053e-01,\n",
      "          1.0554e+00, -3.9841e+00,  3.4206e+00,  2.0370e+00, -6.9422e-01,\n",
      "         -1.2255e+00, -8.7527e-01,  3.7614e-01, -8.7595e-01,  1.9444e+00,\n",
      "          4.1814e-01,  3.3840e-01, -1.3543e+00,  1.2393e+00,  5.7219e-01,\n",
      "         -1.8716e+00, -9.2237e-01,  8.7474e-01,  1.7540e+00, -1.2499e+00,\n",
      "         -4.6632e+00,  7.8118e-01, -1.0743e+00, -5.1267e-02, -7.0192e-01,\n",
      "         -1.3288e+00, -2.4680e+00,  1.1613e+00,  1.0035e+00, -7.5975e-01,\n",
      "          1.0430e+00,  1.5825e+00, -2.5114e+00, -4.5019e-03,  6.0920e-01,\n",
      "          1.8560e+00,  7.5220e-01, -1.9131e+00, -4.2729e-01,  1.5255e+00,\n",
      "         -1.6888e+00, -8.3155e-01, -1.1801e+00, -3.1524e+00,  2.5807e+00,\n",
      "         -1.7624e+00, -1.3248e+00,  9.0805e-01, -2.2556e+00,  1.3618e+00,\n",
      "         -8.7709e-02, -1.0943e+00, -2.5651e+00,  2.0466e-01,  2.4758e+00,\n",
      "         -7.1558e-02, -1.1102e+00,  1.6252e+00, -2.0086e-01, -2.6999e-01,\n",
      "         -1.5182e+00,  6.8117e-01, -2.6573e+00, -1.4574e+00, -3.0269e+00,\n",
      "         -1.0199e-01,  1.7675e-01, -5.9162e-01, -5.7233e-01, -5.3694e-01,\n",
      "          2.8471e+00,  4.2525e+00,  3.6864e-01,  1.5773e+00, -3.0518e-01,\n",
      "          1.0073e+00,  3.3963e-02,  5.0704e-01, -6.2171e-01,  1.1979e+00,\n",
      "         -3.6369e+00, -1.0867e+00, -1.3725e+00, -4.2079e-01, -8.7495e-01,\n",
      "          1.4894e+00,  1.0849e+00, -2.0451e+00,  2.1447e+00, -1.6349e+00,\n",
      "         -1.3236e+00,  2.1147e+00, -7.2420e-01,  4.9080e+00,  2.2132e-01,\n",
      "         -2.0489e+00, -2.8825e+00, -7.7427e-01, -1.5071e+00, -1.9649e+00,\n",
      "         -2.3447e+00, -1.2138e-01, -8.7141e-01, -1.2137e+00, -5.6758e-01,\n",
      "         -1.1459e+00,  3.3920e+00, -7.8559e-01,  4.9209e-02, -1.0171e+00,\n",
      "          1.4342e+00,  1.8941e+00, -2.3422e-01,  1.5757e-01, -2.0974e+00,\n",
      "         -1.2504e+00, -7.2257e-01,  2.1207e+00,  6.3666e-01, -1.5944e-01,\n",
      "         -4.1891e-02,  6.8051e+00,  1.0854e+00,  2.3256e-01, -1.5871e+00,\n",
      "          1.5048e+00, -8.7753e-01, -7.8728e-01,  2.0768e+00,  7.6239e-01,\n",
      "          7.6793e-01,  2.0084e+00, -7.7179e-02, -4.9925e-01, -7.7893e-01,\n",
      "         -1.6130e-01,  1.5991e+00, -1.4248e+00, -1.3059e+00,  1.7281e-01,\n",
      "          9.7145e-01,  8.9823e-01, -6.2337e-01, -3.9817e-01,  4.1502e-01,\n",
      "         -9.8213e-01, -1.4040e+00, -7.0632e-01, -1.2185e+00, -1.3812e-01,\n",
      "         -1.7646e+00,  1.2383e+00, -2.4219e-01, -1.2359e+00, -8.5103e-01,\n",
      "          5.0933e-01,  1.5701e+00,  6.1585e-01,  2.5865e-01, -2.2217e+00,\n",
      "          1.2207e+00,  3.4450e-01,  2.6749e+00, -1.5312e+00, -1.2782e+00,\n",
      "          1.3783e+00, -2.9967e-01, -1.6413e+00,  1.8951e+00, -1.5380e+00,\n",
      "          4.9642e-01, -1.0382e+00,  4.0192e-01, -2.8719e-01, -1.7766e-02,\n",
      "          1.1706e-01,  8.5830e-01,  1.9472e+00,  2.0694e+00, -1.0709e+00,\n",
      "         -2.7514e+00, -2.1133e+00,  4.0760e-01,  5.3724e-01,  2.5611e+00,\n",
      "          5.5287e-01,  9.8521e-01, -7.1155e-01,  7.7967e-01, -2.4560e-01,\n",
      "          5.1646e-01,  3.9960e-01, -6.5054e-01, -9.8002e-01,  1.4670e+00,\n",
      "         -3.6042e+00,  1.7343e+00,  2.3961e+00, -1.1564e+00, -2.6851e-02,\n",
      "         -9.3224e-01,  9.5243e-02,  1.5238e+00, -1.1886e-02, -3.9201e+00,\n",
      "          9.6039e-01, -1.8622e+00,  4.0342e-01,  1.2467e+00, -1.3270e+00,\n",
      "          2.8181e+00, -1.2843e+00,  1.0465e-01, -1.0915e+00, -1.1883e-01,\n",
      "         -2.3449e+00,  2.3037e+00,  1.4105e-01,  1.1762e+00,  2.0731e+00,\n",
      "          3.1626e+00, -4.0518e-01, -2.0531e+00,  3.5942e-01,  7.8084e-01,\n",
      "         -1.4308e+00,  1.9098e+00, -3.4415e-01, -1.5598e+00, -1.0069e+00,\n",
      "          1.6845e+00,  9.4329e-01, -1.1997e-01, -9.8211e-01,  4.9397e-01,\n",
      "         -7.6235e-02, -8.3257e-01,  2.2648e-01,  4.3039e+00,  7.2399e-01,\n",
      "         -1.6925e+00,  4.1639e-01,  2.3661e+00,  5.7614e-01, -7.0501e-01,\n",
      "          9.3698e-01,  3.1564e+00,  1.3448e+00, -1.1791e+00, -2.3946e+00,\n",
      "          1.0355e+00, -1.4283e+00, -3.0841e-01, -2.0985e-01,  7.5719e-01,\n",
      "         -5.9639e-01, -3.3182e+00, -4.7600e-01,  4.4860e-01, -9.0679e-02,\n",
      "          3.6439e-01, -1.2680e+00,  4.2017e+00,  2.5687e-01,  2.8891e+00,\n",
      "          4.6001e-01,  4.0839e-01,  1.1886e-01, -3.5359e-01,  1.6767e+00,\n",
      "         -4.5943e-02,  6.5104e-01, -1.4483e+00,  3.1760e+00,  3.7040e+00,\n",
      "         -2.8563e+00, -3.1423e+00, -7.3179e-01, -1.7113e+00,  7.4204e-01,\n",
      "          4.2495e-01, -1.6541e-01,  1.2043e+00, -2.2778e+00, -4.0974e-01,\n",
      "          3.2469e+00,  5.4545e-01,  4.0701e+00,  5.6545e-02, -7.1803e-01,\n",
      "          1.4253e+00, -1.3938e-01, -5.1372e-01,  5.1557e-01,  9.8734e-01,\n",
      "         -2.1815e+00, -7.1921e-01,  1.0509e+00, -1.7259e+00, -3.9722e-01,\n",
      "          2.5960e+00,  3.0269e-01,  1.7854e+00, -6.9886e-01, -9.2756e-01,\n",
      "         -3.6808e-01, -2.2924e+00, -2.3406e+00,  2.0640e-01, -2.6725e+00,\n",
      "         -1.5565e+00,  5.5603e-01,  2.4937e-02, -7.0716e-01,  2.5405e+00,\n",
      "         -1.5649e+00, -2.4955e+00,  2.2369e+00,  6.6260e-02, -2.2185e+00,\n",
      "          2.1173e-01,  1.0815e+00, -8.8297e-01, -1.6948e+00, -5.3696e-01,\n",
      "         -2.9586e-01,  9.3007e-01,  5.0781e-01, -5.0682e+00, -3.0134e+00,\n",
      "          1.1831e+00,  1.3424e+00, -3.5084e-01,  2.6807e+00, -1.3913e-01,\n",
      "         -9.3794e-01,  1.4302e-01, -3.8884e+00,  7.5477e-01,  1.3041e+00,\n",
      "         -1.4114e+00,  2.0522e+00, -1.1914e+00, -3.6951e+00, -1.1268e+00,\n",
      "          1.1936e+00, -1.7634e+00, -3.2849e-01, -2.1862e-01,  1.2523e+00,\n",
      "          2.7501e-01, -1.8718e+00,  1.7885e+00, -4.1563e+00, -4.4140e-01,\n",
      "          1.1588e+00, -7.5448e-03, -2.8474e+00, -1.9043e+00, -2.4453e+00,\n",
      "          2.5315e-01,  8.8875e-02,  1.6344e+00,  4.4536e-01, -4.7590e-01,\n",
      "         -2.0069e+00, -5.2142e-01,  6.1518e-01, -5.6937e-01, -1.3218e+00,\n",
      "         -1.4279e-02, -9.6212e-01, -1.6103e+00, -7.1548e-01, -1.3822e+00,\n",
      "          2.3151e+00, -5.5095e-01,  1.0492e+00, -4.0120e-01, -1.3629e+00,\n",
      "         -1.6552e+00,  8.4025e+00, -9.9357e-01, -2.8702e-01,  3.7297e-01,\n",
      "          6.5176e-01,  2.7034e-01,  8.3489e-01,  6.4726e-01, -2.3133e+00,\n",
      "         -2.6451e+00, -6.9854e-01,  3.4881e-01, -5.3249e-01,  1.2456e+00,\n",
      "         -6.7397e-02, -1.6766e+00,  1.5877e+00, -6.4095e-01, -2.1274e-01,\n",
      "          7.2771e-01,  2.2252e-01,  4.0577e+00, -1.8594e+00, -1.6097e+00,\n",
      "         -1.5720e-01, -1.5695e+00,  3.6964e-01,  1.2944e+00, -3.2141e+00,\n",
      "         -1.8944e+00, -1.9046e+00,  1.3706e+00,  1.6082e-01, -1.1027e+00,\n",
      "          3.0163e-01,  1.9843e-01, -1.9103e+00,  9.6125e-01,  7.7401e-01,\n",
      "          8.3753e-01,  1.0548e+00,  2.2444e+00,  2.0095e+00,  7.4966e-01,\n",
      "          6.0005e-02, -1.6765e+00, -1.4351e-01, -3.8011e+00,  1.4849e+00,\n",
      "         -1.8601e+00,  3.7222e-01,  1.2112e+00,  1.0047e-01, -1.5455e+00,\n",
      "          3.3825e+00,  5.6913e-01, -7.5946e-01, -1.6222e+00,  2.5421e+00,\n",
      "          1.5802e-01,  4.8031e-02, -5.0369e-01, -9.8650e-01, -1.0688e+00,\n",
      "          1.2588e+00,  5.3709e+00,  2.6321e+00,  2.0898e+00,  5.0968e-01,\n",
      "          7.3088e-01, -2.5586e+00,  5.9155e-01, -1.3207e-01, -4.4336e-01,\n",
      "          1.5474e+00, -6.4305e-01,  6.3285e-01,  1.5812e+00, -1.0640e+00,\n",
      "         -7.7059e+00,  2.5250e-01, -6.8611e-01,  1.8897e-01, -4.7380e-01,\n",
      "         -8.3959e-01, -9.8897e-02, -1.7588e+00, -1.6120e-01,  1.4693e+00,\n",
      "         -1.0136e+00, -3.5969e-01,  1.7872e-01,  4.4836e-01, -1.2212e+00,\n",
      "          1.4090e+00,  7.8492e-01,  2.6057e+00, -1.9053e+00,  8.5539e-02,\n",
      "          5.9310e-01, -1.3523e+00, -1.1363e+01,  2.4137e-01, -2.6495e+00,\n",
      "         -1.4864e+00,  2.4880e+00, -1.8296e+00,  9.6969e-01, -2.9195e+01,\n",
      "         -1.3179e+00,  4.5808e-02, -1.4789e+00, -1.2261e+00,  2.9111e-01,\n",
      "          1.8506e+00,  2.0197e-02, -6.2679e-01, -2.8944e-01,  1.8636e+00,\n",
      "          1.2678e+01,  1.5241e+00,  1.5953e+00, -2.4619e+00, -2.2728e-01,\n",
      "         -4.5266e-02,  4.2161e-01, -1.2637e+00,  1.1179e+00,  1.0318e+01,\n",
      "         -2.6648e-02,  5.3194e-01, -5.7089e-01,  9.0061e-01, -5.2707e-01,\n",
      "         -2.0982e+00,  9.9255e-01,  5.5476e-01,  2.1142e+00,  2.4852e+00,\n",
      "         -1.8135e+00, -7.1060e-02,  1.0012e+00, -1.3134e+00,  1.1842e-01,\n",
      "          1.0261e-01,  1.5901e+00, -4.4797e-03, -2.3698e+00,  2.6646e+00,\n",
      "          3.1567e-01,  2.0947e+00, -8.9459e-01,  2.5806e-02, -3.7621e-01,\n",
      "         -3.7301e-01,  1.8946e+00, -9.0984e-01,  9.5997e-01,  1.3598e-01,\n",
      "          1.7907e+00,  5.8172e-01,  3.1967e+00,  1.1874e+00, -5.3881e-01,\n",
      "          2.1945e-01, -2.5187e-01, -1.0605e+00, -1.1575e+00, -1.8743e+00,\n",
      "          6.6341e-01,  1.7279e-01, -2.7946e+00,  2.0029e-01,  6.7683e-01,\n",
      "         -2.5205e+00,  2.9886e+00,  2.7558e+00,  1.4878e-01, -1.7041e+00,\n",
      "          7.7236e-01,  2.0746e+00,  1.5717e+00, -2.7782e+00,  2.6493e+00,\n",
      "         -1.7469e+00,  1.0527e+00,  2.6882e+00,  2.3561e+01,  7.2367e-03,\n",
      "          1.9415e+00,  3.7905e-01, -1.0160e+00, -1.4798e+00,  8.1134e-01,\n",
      "         -4.5316e-01, -1.2766e-02, -5.1397e-01,  3.6397e-01,  7.6735e-01,\n",
      "         -7.4074e-01, -3.3751e+00,  1.1789e+00, -5.9358e-01,  4.0161e-01,\n",
      "         -1.6574e+00, -4.1042e-02,  1.5815e+00,  1.9908e-01, -8.9331e-01,\n",
      "          6.8684e-01,  1.7786e+00, -9.9205e-01,  5.9394e-03,  1.3169e+00,\n",
      "         -3.5673e-01,  1.3827e+00,  5.8803e-01,  1.2071e+01, -2.1162e+00,\n",
      "          7.5674e-01, -1.0384e+00, -1.7524e-01, -4.9897e-01,  8.8172e-01,\n",
      "         -1.1214e+00,  2.1206e-02, -2.0963e+01, -5.4716e-01,  2.3314e+00,\n",
      "         -5.8861e-01, -8.2573e-01, -1.9344e-01, -2.6478e+00,  2.6776e+00,\n",
      "         -9.4633e-01,  1.2581e+00,  4.9739e-01,  2.0614e+00,  8.2128e-01,\n",
      "          5.1752e+00, -1.9135e+00,  2.2645e-01, -1.1569e+00, -5.1545e-01,\n",
      "          1.4950e+00, -2.1308e+00, -2.4925e-01,  6.8795e-01,  8.2867e-01,\n",
      "          1.4657e+00,  8.3450e-01, -1.1875e+00, -2.4893e+00, -7.6399e-01,\n",
      "         -1.7120e-01, -2.3673e+00, -2.5387e+00, -1.5402e+00, -1.4348e+00,\n",
      "          2.3930e+00, -8.1171e-01, -1.7630e+00, -9.8690e-01, -5.4944e-04,\n",
      "          2.8757e+00, -2.4585e-01,  1.7632e+00,  6.6187e-01,  2.2768e+00,\n",
      "          3.3118e+00, -8.2274e-02,  1.0288e+00,  9.4805e-01, -2.3525e+00,\n",
      "         -1.4872e-01, -4.1888e-01, -4.5357e+00,  7.7222e-01, -6.4544e-01,\n",
      "         -2.0682e+00,  1.0918e+00,  6.2440e-03,  6.7083e-01,  2.0939e+00,\n",
      "          1.2003e+00, -1.2521e+00,  1.2331e+00, -2.3861e+00,  5.4378e-01,\n",
      "         -2.6325e-01,  1.2434e+00,  1.3721e+00, -2.2784e+00, -1.2062e+00,\n",
      "          3.6625e+00,  8.0851e-02,  1.9509e+00, -1.0045e+00, -1.2415e+00,\n",
      "          2.5469e-01, -1.4610e+00, -8.6098e-01, -2.6494e-02, -1.2287e+00,\n",
      "         -9.2025e-01, -1.7983e+00, -8.3840e-01, -5.9564e-01, -3.7165e+00,\n",
      "          2.2492e+00,  1.4471e+00,  9.9051e-01,  1.0652e+00,  1.2004e-01,\n",
      "         -1.2140e+00,  3.8493e+00,  5.7785e-02, -4.1316e+00, -1.4069e+00,\n",
      "         -1.1746e+00, -6.2042e-01,  1.3250e-01, -4.9906e-01, -3.2170e+00,\n",
      "          1.6580e+00,  3.0954e+00, -7.6075e-01,  1.9511e+00,  5.5135e-01,\n",
      "         -9.8865e-01, -1.3666e-01,  6.5454e-01,  1.2260e+00,  3.2961e-01,\n",
      "         -2.2699e+00,  6.6429e-01,  6.1729e-01,  3.2996e-01, -2.5743e+00,\n",
      "          1.5865e-01, -8.3394e-01,  2.7251e-01, -2.2424e+00,  4.4020e-01,\n",
      "         -1.5239e+00,  1.7981e+00, -6.0964e-01]])\n",
      "Class: filtering\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/culbec/.conda/envs/myenv/lib/python3.12/site-packages/transformers/generation/configuration_utils.py:563: UserWarning: `num_beams` is set to 1. However, `early_stopping` is set to `True` -- this flag is only used in beam-based generation modes. You should set `num_beams>1` or unset `early_stopping`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Code along with comments:\n",
      " \n",
      "    You are a software specification generator. Given the following code snippet, generate a detailed specification comment that includes:\n",
      "    1. Purpose of the code.\n",
      "    2. Short description of the logic of each function and method.\n",
      "    3. Description of the input parameters and the output.\n",
      "    4. Include complexity analysis if possible.\n",
      "    \n",
      "    Code Snippet:\n",
      "    def f(text: str):\n",
      "    words = text.split(\" \")\n",
      "\n",
      "    word = words[0]\n",
      "\n",
      "    for i in range(1, len(words)):\n",
      "        if words[i] > word:\n",
      "            word = words[i]\n",
      "\n",
      "    return word\n",
      "    \n",
      "    Specification:\n",
      "     - The first line will contain all information about it's definition (excluding basic keywords) or its constructor signature with any additional details as needed; more specifically name/class attributes like names must be included before keyword arguments on their own lines to provide support documentation\n",
      "       Example usage is here :  class Foo {\n",
      "           foo().a() # returns value\n",
      "             -> Returns string \"Foo\"\n",
      "\n",
      "             @param x=int\n",
      "         }\n",
      "\n",
      "\n",
      "        <BLANKLINE>\n",
      "         ...other example goes below..\n",
      "\n",
      "       </LICENSE><EOS></License>, see also self.__doc__ above\n",
      "\n",
      "   This file should include an implementation detail which was directly created by this module under license\n",
      "\"\"\"\n",
      "\n",
      "\n",
      "Request:\n",
      " Determine the kth largest element of a sequence of numbers with n elements (k < n). For example. the 2nd largest element in the string [7,4,6,3,9,1] is 7.\n",
      "\n",
      "\n",
      "Code of request:\n",
      " Determine the kth largest element of a sequence of numbers with n elements (k < n). For example. the 2nd largest element in the string [7,4,6,3,9,1] is 7. The 3rd smallest from this list: 5 = 1 and 6 > 4 are 0\n",
      "#   >>> l=[2,-5],l=sorted(enumerate([8]))[-20:] # returns [[0],[14]]\n",
      "\n",
      "\treturn sorted([(i - j) for i,j  pair ])[len(_list)-22-max(-_range)].index() + max([_num])<|endoftext|>\n"
     ]
    }
   ],
   "source": [
    "code = load_code()\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"Daoguang/PyCodeGPT\")\n",
    "model = AutoModel.from_pretrained(\"Daoguang/PyCodeGPT\")\n",
    "device = torch.device('cpu')\n",
    "\n",
    "data = load_dataset(\"data/dataset.csv\")\n",
    "labels = data['label']\n",
    "\n",
    "x_train, x_test, y_train, y_test = split_data(data, test_size=0.1)\n",
    "x_train, x_test = get_embeddings(x_train, x_test, tokenizer, model, device)\n",
    "classifier = RandomForestClassifier()\n",
    "\n",
    "train_classifier(classifier, x_train, y_train)\n",
    "\n",
    "print(f\"Base code:\\n {code}\\n\\n\")\n",
    "print(f\"Embeddings: {generate_embeddings(code, tokenizer, model, device)}\")\n",
    "print(f\"Class: {classify_code(classifier, generate_embeddings(code, tokenizer, model, device))}\\n\\n\")\n",
    "\n",
    "model_lm = GPTNeoForCausalLM.from_pretrained(\"Daoguang/PyCodeGPT\")\n",
    "print(f\"Code along with comments:\\n {generate_comments(code, tokenizer, model_lm)}\")\n",
    "\n",
    "request = load_request()\n",
    "print(f\"Request:\\n {request}\\n\\n\")\n",
    "print(f\"Code of request:\\n {generate_from_request(request, tokenizer, model_lm, max_length=1024)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
