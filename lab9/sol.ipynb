{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import time\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.cluster import KMeans, AgglomerativeClustering, DBSCAN\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "from gensim.scripts.glove2word2vec import glove2word2vec\n",
    "from gensim.models import KeyedVectors, FastText\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.ai.textanalytics import TextAnalyticsClient\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from kmeans import KMeansClustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data processing\n",
    "\n",
    "- Loading the data -> `text` and `sentiment/type`.\n",
    "- Splitting the data into `train` and `test` samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(data_file: str) -> tuple[list, list, list]:\n",
    "    cwd = os.getcwd()\n",
    "    file_path = os.path.join(cwd, \"data\", data_file)\n",
    "    \n",
    "    data = pd.read_csv(file_path)\n",
    "    \n",
    "    inputs = [el for el in data.iloc[:, 0]]\n",
    "    outputs = [el for el in data.iloc[:, 1]]\n",
    "    labels = list(set(outputs))\n",
    "    \n",
    "    return inputs, outputs, labels\n",
    "\n",
    "\n",
    "def load_go_emotions(data_file: str) -> tuple[list, list, list]:\n",
    "    cwd = os.getcwd()\n",
    "    file_path = os.path.join(cwd, \"data\", data_file)\n",
    "    \n",
    "    data = pd.read_csv(file_path)\n",
    "    emotion_cols = data.columns[9:]\n",
    "    \n",
    "    def get_emotion_from_multi_label(row):\n",
    "        for c in emotion_cols:\n",
    "            if row[c] == 1:\n",
    "                return c\n",
    "        return \"neutral\"\n",
    "    \n",
    "    data['emotion'] = data.apply(get_emotion_from_multi_label, axis=1)\n",
    "    inputs = [el for el in data.iloc[:, 0]]\n",
    "    outputs = [el for el in data.iloc[:, -1]]\n",
    "    labels = list(set(outputs))\n",
    "    \n",
    "    return inputs, outputs, labels\n",
    "    \n",
    "\n",
    "def split_data(inputs: list, outputs: list) -> tuple[list, list, list, list]:\n",
    "    X_train, X_test, y_train, y_test = train_test_split(inputs, outputs, test_size=0.2, random_state=42, shuffle=True)\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word vectorizing\n",
    "\n",
    "Vectorizing the passed sentences into vectors using:\n",
    "\n",
    "- `Bag of Words` -> by `CountVectorizer`.\n",
    "- `TF-IDF` -> by `TfidfVectorizer`.\n",
    "- `Word2Vec` -> by `GoogleNews model`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bag_of_words(train: list, test: list):\n",
    "    vectorizer = CountVectorizer()\n",
    "    \n",
    "    train_features = vectorizer.fit_transform(train)\n",
    "    test_features = vectorizer.transform(test)\n",
    "    \n",
    "    # print(\"Vocabulary size: \", vectorizer.vocabulary_, \" words\")\n",
    "    # print(\"Train size: \", len(train), \" emails\")\n",
    "    # print(\"Train shape: \", train_features.shape)\n",
    "    \n",
    "    return train_features, test_features\n",
    "\n",
    "def tf_idf(train: list, test: list):\n",
    "    vectorizer = TfidfVectorizer(max_features=50)\n",
    "    \n",
    "    train_features = vectorizer.fit_transform(train)\n",
    "    test_features = vectorizer.transform(test)\n",
    "    \n",
    "    # print(\"Vocabulary size: \", vectorizer.vocabulary_, \" words\")\n",
    "    # print(\"Train size: \", len(train), \" emails\")\n",
    "    # print(\"Train shape: \", train_features.shape)\n",
    "    \n",
    "    return train_features, test_features\n",
    "\n",
    "def sentence_to_vec(sentence: str, model: KeyedVectors) -> np.array:\n",
    "    words = word_tokenize(sentence)\n",
    "    feature_vec = np.zeros((model.vector_size,), dtype=\"float32\")\n",
    "    no_words = 0\n",
    "    \n",
    "    for word in words:\n",
    "        if word in model:\n",
    "            no_words += 1\n",
    "            feature_vec = np.add(feature_vec, model[word])\n",
    "    \n",
    "    if no_words > 0:\n",
    "        feature_vec = np.divide(feature_vec, no_words)\n",
    "    \n",
    "    return feature_vec\n",
    "\n",
    "def word2vec(train: list, test: list, model: KeyedVectors) -> tuple[list, list]:\n",
    "    train_features = [sentence_to_vec(s, model) for s in train]\n",
    "    test_features = [sentence_to_vec(s, model) for s in test]\n",
    "    \n",
    "    return train_features, test_features\n",
    "\n",
    "def fast_text(train: list, test: list, model: FastText) -> tuple[list, list]:\n",
    "    tr_norm = [model.wv[s] for s in train]\n",
    "    ts_norm = [model.wv[s] for s in test]\n",
    "    \n",
    "    return tr_norm, ts_norm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying\n",
    "\n",
    "Classifying sentences contained in a given file, along with the initial classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kmeans_tool(train: list, test: list, labels: list) -> tuple[list, list]:\n",
    "    classifier = KMeans(n_clusters=len(labels), init='k-means++', random_state=42)\n",
    "    \n",
    "    # Fitting the data.\n",
    "    classifier.fit(train)\n",
    "    \n",
    "    # Predicting the outputs based on the test features.\n",
    "    # Also showing the prediction for the train features.\n",
    "    predictedIndexesTrain = classifier.predict(train)\n",
    "    predictedIndexesTest = classifier.predict(test)\n",
    "    predicted_train = [labels[value] for value in predictedIndexesTrain]\n",
    "    predicted_test = [labels[value] for value in predictedIndexesTest]\n",
    "    \n",
    "    return predicted_train, predicted_test\n",
    "\n",
    "def agglomerative_tool(train: list, test: list, labels: list) -> tuple[list, list]:\n",
    "    classifier = AgglomerativeClustering(n_clusters=len(labels))\n",
    "    \n",
    "    # Fitting the data.\n",
    "    classifier.fit(train)\n",
    "    \n",
    "    # Predicting the outputs based on the test features.\n",
    "    # Also showing the prediction for the train features.\n",
    "    predictedIndexesTrain = classifier.labels_\n",
    "    predictedIndexesTest = classifier.fit_predict(test)\n",
    "    predicted_train = [labels[value] for value in predictedIndexesTrain]\n",
    "    predicted_test = [labels[value] for value in predictedIndexesTest]\n",
    "    \n",
    "    return predicted_train, predicted_test\n",
    "\n",
    "def dbscan_tool(train: list, test: list, labels: list) -> tuple[list, list]:\n",
    "    # Create an instance of HDBSCAN\n",
    "    classifier = DBSCAN(eps=0.5, min_samples=5)\n",
    "    \n",
    "    # Fitting the data.\n",
    "    classifier.fit(train)\n",
    "    \n",
    "    # Predicting the outputs based on the test features.\n",
    "    # Also showing the prediction for the train features.\n",
    "    predictedIndexesTrain = classifier.labels_\n",
    "    predictedIndexesTest = classifier.fit_predict(test)\n",
    "    predicted_train = [labels[value] for value in predictedIndexesTrain]\n",
    "    predicted_test = [labels[value] for value in predictedIndexesTest]\n",
    "    \n",
    "    return predicted_train, predicted_test\n",
    "\n",
    "def my_kmeans(train: list, test: list, labels: list) -> tuple[list, list]:\n",
    "    classifier = KMeansClustering(k=len(labels))\n",
    "    \n",
    "    # Fitting the data.\n",
    "    classifier.fit(train)\n",
    "    \n",
    "    # Predicting the outputs based on the test features.\n",
    "    # Also showing the prediction for the train features.\n",
    "    predictedIndexesTrain = classifier.predict(train)\n",
    "    predictedIndexesTest = classifier.predict(test)\n",
    "    predicted_train = [labels[value] for value in predictedIndexesTrain]\n",
    "    predicted_test = [labels[value] for value in predictedIndexesTest]\n",
    "    \n",
    "    return predicted_train, predicted_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing and Scoring\n",
    "\n",
    "Visualizing the predicted outputs with respect to the actual test outputs.\n",
    "Scoring the predicted outputs using `sklearn`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize(test: list, predicted: list, actual: list) -> None:\n",
    "    for i, sentence in enumerate(test):\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        print(f\"Predicted: {predicted[i]} | Actual: {actual[i]}\\n\")\n",
    "        \n",
    "def score(predicted: list, actual: list):\n",
    "    return accuracy_score(actual, predicted)\n",
    "\n",
    "def plot_clusters(X: list, labels: list, kmeans: KMeansClustering) -> None:\n",
    "    plt.scatter(X[:, 0], X[:, 1], c=labels)\n",
    "    plt.scatter(kmeans.centroids[:, 0], kmeans.centroids[:, 1], c=range(len(kmeans.centroids)),\n",
    "            marker=\"*\", s=200)\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Azure Language Model\n",
    "\n",
    "Using the `Azure` language model to classify text emotions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_client() -> TextAnalyticsClient:\n",
    "    endpoint = os.environ[\"LANGUAGE_ENDPOINT\"]\n",
    "    key = os.environ[\"LANGUAGE_KEY\"]\n",
    "    \n",
    "    credential = AzureKeyCredential(key)\n",
    "    \n",
    "    client = TextAnalyticsClient(endpoint=endpoint, credential=credential)\n",
    "    return client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_bag_of_words(file_name: str) -> None:\n",
    "    inputs, outputs, labels = load_data(file_name)\n",
    "    \n",
    "    Xtr, Xts, ytr, yts = split_data(inputs, outputs)\n",
    "    \n",
    "    Xtr_norm, Xts_norm = bag_of_words(Xtr, Xts)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    predicted_train, predicted_test = kmeans_tool(Xtr_norm, Xts_norm, labels)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"Bag of Words\\n\\n\")\n",
    "    # visualize(Xtr, predicted_train, ytr)\n",
    "    # print(f\"Accuracy: {score(predicted_train, ytr)} on {len(Xtr)} train samples\")\n",
    "    \n",
    "    visualize(Xts, predicted_test, yts)\n",
    "    print(f\"Accuracy: {score(predicted_test, yts)} on {len(Xts)} test samples in {end_time - start_time} ms\")\n",
    "    \n",
    "def classify_tf_idf(file_name: str) -> None:\n",
    "    inputs, outputs, labels = load_data(file_name)\n",
    "    \n",
    "    Xtr, Xts, ytr, yts = split_data(inputs, outputs)\n",
    "    \n",
    "    Xtr_norm, Xts_norm = tf_idf(Xtr, Xts)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    predicted_train, predicted_test = kmeans_tool(Xtr_norm, Xts_norm, labels)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"TF-IDF\\n\\n\")\n",
    "    # visualize(Xtr, predicted_train, ytr)\n",
    "    # print(f\"Accuracy: {score(predicted_train, ytr)} on {len(Xtr)} train samples\")\n",
    "    \n",
    "    visualize(Xts, predicted_test, yts)\n",
    "    print(f\"Accuracy: {score(predicted_test, yts)} on {len(Xts)} test samples in {end_time - start_time} ms\")\n",
    "    \n",
    "def classify_word2vec(file_name: str) -> None:\n",
    "    inputs, outputs, labels = load_data(file_name)\n",
    "    \n",
    "    Xtr, Xts, ytr, yts = split_data(inputs, outputs)\n",
    "    \n",
    "    # Building the word2vec model.\n",
    "    cwd = os.getcwd()\n",
    "    modelPath = os.path.join(cwd, \"models\", \"GoogleNews-vectors-negative300.bin\")\n",
    "    word2vec300Model = KeyedVectors.load_word2vec_format(modelPath, binary=True)\n",
    "    \n",
    "    Xtr_norm, Xts_norm = word2vec(Xtr, Xts, word2vec300Model)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #predicted_train, predicted_test = kmeans_tool(Xtr_norm, Xts_norm, labels)\n",
    "    predicted_train, predicted_test = my_kmeans(Xtr_norm, Xts_norm, labels)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"Word2Vec\\n\\n\")\n",
    "    # visualize(Xtr, predicted_train, ytr)\n",
    "    # print(f\"Accuracy: {score(predicted_train, ytr)} on {len(Xtr)} train samples\")\n",
    "    \n",
    "    visualize(Xts, predicted_test, yts)\n",
    "    print(f\"Accuracy: {score(predicted_test, yts)} on {len(Xts)} test samples in {end_time - start_time} ms\")\n",
    "    \n",
    "def classify_azure(file_name: str) -> None:\n",
    "    inputs, outputs, labels = load_data(file_name)\n",
    "    \n",
    "    Xtr, Xts, ytr, yts = split_data(inputs, outputs)\n",
    "\n",
    "    client = get_client()\n",
    "    \n",
    "    result_tr = client.analyze_sentiment(Xtr[:10], show_opinion_mining=True)\n",
    "    docs_tr = [doc for doc in result_tr if not doc.is_error]\n",
    "    \n",
    "    # Showing train sentiments.\n",
    "    for i, doc in enumerate(docs_tr):\n",
    "        print(f\"Text: {Xtr[i]}\")\n",
    "        print(f\"Predicted: {doc.sentiment} | Actual: {ytr[i]}\\n\")\n",
    "        \n",
    "    \n",
    "    result_ts = client.analyze_sentiment(Xts[:10], show_opinion_mining=True)\n",
    "    docs_ts = [doc for doc in result_ts if not doc.is_error]\n",
    "    \n",
    "    # Showing test sentiments.\n",
    "    for i, doc in enumerate(docs_ts):\n",
    "        print(f\"Text: {Xts[i]}\")\n",
    "        print(f\"Predicted: {doc.sentiment} | Actual: {yts[i]}\\n\")\n",
    "        \n",
    "def classify_go_emotions(file_name: str) -> None:\n",
    "    inputs, outputs, labels = load_go_emotions(file_name)\n",
    "    \n",
    "    Xtr, Xts, ytr, yts = split_data(inputs, outputs)\n",
    "    \n",
    "    Xtr_norm, Xts_norm = tf_idf(Xtr, Xts)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    predicted_train, predicted_test = kmeans_tool(Xtr_norm, Xts_norm, labels)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"TF-IDF\\n\\n\")\n",
    "    # visualize(Xtr, predicted_train, ytr)\n",
    "    # print(f\"Accuracy: {score(predicted_train, ytr)} on {len(Xtr)} train samples\")\n",
    "    \n",
    "    visualize(Xts, predicted_test, yts)\n",
    "    print(f\"Accuracy: {score(predicted_test, yts)} on {len(Xts)} test samples in {end_time - start_time} ms\")\n",
    "    \n",
    "def classify_fast_text(file_name: str) -> None:\n",
    "    inputs, outputs, labels = load_data(file_name)\n",
    "    \n",
    "    Xtr, Xts, ytr, yts = split_data(inputs, outputs)\n",
    "    \n",
    "    # Building the FastText model.\n",
    "    tokenized_sentences = [word_tokenize(s) for s in inputs]\n",
    "    model = FastText(tokenized_sentences, vector_size=100, min_count=1, window=5, workers=8)\n",
    "    \n",
    "    Xtr_norm, Xts_norm = fast_text(Xtr, Xts, model)\n",
    "    \n",
    "    #predicted_train, predicted_test = kmeans_tool(Xtr_norm, Xts_norm, labels)\n",
    "    start_time = time.time()\n",
    "    predicted_train, predicted_test = my_kmeans(Xtr_norm, Xts_norm, labels)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"FastText\\n\\n\")\n",
    "    # visualize(Xtr, predicted_train, ytr)\n",
    "    # print(f\"Accuracy: {score(predicted_train, ytr)} on {len(Xtr)} train samples\")\n",
    "    \n",
    "    visualize(Xts, predicted_test, yts)\n",
    "    print(f\"Accuracy: {score(predicted_test, yts)} on {len(Xts)} test samples in {end_time - start_time} ms\")\n",
    "    \n",
    "def classify_glove(file_name: str) -> None:\n",
    "    inputs, outputs, labels = load_data(file_name)\n",
    "    \n",
    "    Xtr, Xts, ytr, yts = split_data(inputs, outputs)\n",
    "    \n",
    "    # Building the word2vec model.\n",
    "    cwd = os.getcwd()\n",
    "    modelPath = os.path.join(cwd, \"models\", \"glove.6B.300d.txt\")\n",
    "    word2vec_output = 'glove.6B.300d.txt.word2vec'\n",
    "    \n",
    "    glove2word2vec(modelPath, word2vec_output)\n",
    "    word2vecModel = KeyedVectors.load_word2vec_format(word2vec_output, binary=True)\n",
    "    \n",
    "    Xtr_norm, Xts_norm = word2vec(Xtr, Xts, word2vecModel)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #predicted_train, predicted_test = kmeans_tool(Xtr_norm, Xts_norm, labels)\n",
    "    predicted_train, predicted_test = my_kmeans(Xtr_norm, Xts_norm, labels)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"GloVe\\n\\n\")\n",
    "    # visualize(Xtr, predicted_train, ytr)\n",
    "    # print(f\"Accuracy: {score(predicted_train, ytr)} on {len(Xtr)} train samples\")\n",
    "    \n",
    "    visualize(Xts, predicted_test, yts)\n",
    "    print(f\"Accuracy: {score(predicted_test, yts)} on {len(Xts)} test samples in {end_time - start_time} ms\")\n",
    "    \n",
    "def classify_agglomerative(file_name: str) -> None:\n",
    "    inputs, outputs, labels = load_data(file_name)\n",
    "    \n",
    "    Xtr, Xts, ytr, yts = split_data(inputs, outputs)\n",
    "    \n",
    "    Xtr_norm, Xts_norm = bag_of_words(Xtr, Xts)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    predicted_train, predicted_test = agglomerative_tool(Xtr_norm, Xts_norm, labels)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"Agglomerative Clustering\\n\\n\")\n",
    "    # visualize(Xtr, predicted_train, ytr)\n",
    "    # print(f\"Accuracy: {score(predicted_train, ytr)} on {len(Xtr)} train samples\")\n",
    "    \n",
    "    visualize(Xts, predicted_test, yts)\n",
    "    print(f\"Accuracy: {score(predicted_test, yts)} on {len(Xts)} test samples in {end_time - start_time} ms\")\n",
    "    \n",
    "def classify_dbscan(file_name: str) -> None:\n",
    "    inputs, outputs, labels = load_data(file_name)\n",
    "    \n",
    "    Xtr, Xts, ytr, yts = split_data(inputs, outputs)\n",
    "    \n",
    "    Xtr_norm, Xts_norm = bag_of_words(Xtr, Xts)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    predicted_train, predicted_test = dbscan_tool(Xtr_norm, Xts_norm, labels)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(\"DBSCAN\\n\\n\")\n",
    "    # visualize(Xtr, predicted_train, ytr)\n",
    "    # print(f\"Accuracy: {score(predicted_train, ytr)} on {len(Xtr)} train samples\")\n",
    "    \n",
    "    visualize(Xts, predicted_test, yts)\n",
    "    print(f\"Accuracy: {score(predicted_test, yts)} on {len(Xts)} test samples in {end_time - start_time} ms\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_48731/1248938041.py:136: DeprecationWarning: Call to deprecated `glove2word2vec` (KeyedVectors.load_word2vec_format(.., binary=False, no_header=True) loads GLoVE text vectors.).\n",
      "  glove2word2vec(modelPath, word2vec_output)\n"
     ]
    }
   ],
   "source": [
    "file_spam = \"spam.csv\"\n",
    "file_review = \"reviews_mixed.csv\"\n",
    "file_go_emotions = \"goemotions_3.csv\"\n",
    "\n",
    "#classify_bag_of_words(file_review)\n",
    "#classify_tf_idf(file_spam)\n",
    "#classify_word2vec(file_review)\n",
    "#classify_azure(file_review)\n",
    "#classify_go_emotions(file_go_emotions)\n",
    "#classify_fast_text(file_review)\n",
    "classify_glove(file_review)\n",
    "#classify_agglomerative(file_review)\n",
    "#classify_dbscan(file_review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
